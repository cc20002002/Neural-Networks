{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import h5py\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data set shape: (60000, 128)\n",
      "Training label set shape: (60000, 1)\n",
      "Testing data set shape: (10000, 128)\n"
     ]
    }
   ],
   "source": [
    "# Import Training Data and Testing Data\n",
    "with h5py.File('train_128.h5','r') as H: \n",
    "    train_data_set = np.copy(H['data'])  \n",
    "with h5py.File('train_label.h5','r') as H:\n",
    "    train_labels_set = np.copy(H['label'])\n",
    "with h5py.File('test_128.h5', 'r') as H:\n",
    "    test_data_set = np.copy(H['data'])\n",
    "\n",
    "# Reshape train_labels_set to be a \"m x 1\" matrix\n",
    "train_labels_set = train_labels_set.reshape(train_labels_set.shape[0], 1)\n",
    "train_data_set_shape = train_data_set.shape\n",
    "train_labels_set_shape = train_labels_set.shape\n",
    "test_data_set_shape = test_data_set.shape\n",
    "\n",
    "print(f'Training data set shape: {train_data_set_shape}')\n",
    "print(f'Training label set shape: {train_labels_set_shape}')\n",
    "print(f'Testing data set shape: {test_data_set_shape}')\n",
    "\n",
    "learning_rate = 0.11\n",
    "max_iteration = 15\n",
    "droput_rate = 0.92\n",
    "batch_size = 16\n",
    "hidden_layer_dim = 32\n",
    "output_layer_dim = len(set(train_labels_set[:,0]))\n",
    "trainsize = 50048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (50048, 128)\n",
      "Training Labels Shape: (50048, 1)\n",
      "Validation Data Shape: (9952, 128)\n",
      "Validation Labels Shape: (9952, 1)\n"
     ]
    }
   ],
   "source": [
    "# Split Training Sample into Training Dataset and Validation Dataset\n",
    "train_data = train_data_set[0:trainsize,:]\n",
    "train_labels = train_labels_set[0:trainsize,:]\n",
    "validation_data = train_data_set[trainsize:,:]\n",
    "validation_labels = train_labels_set[trainsize:,:]\n",
    "\n",
    "print(f'Training Data Shape: {train_data.shape}')\n",
    "print(f'Training Labels Shape: {train_labels.shape}')\n",
    "print(f'Validation Data Shape: {validation_data.shape}')\n",
    "print(f'Validation Labels Shape: {validation_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Multi-Layer Neural Network Dimensions: [128, 32, 32, 10]\n"
     ]
    }
   ],
   "source": [
    "# Calculate Neural Network Dimensions\n",
    "layer_dims = [train_data.shape[1], hidden_layer_dim, hidden_layer_dim, output_layer_dim]\n",
    "print(f'The Multi-Layer Neural Network Dimensions: {layer_dims}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Different Activation Functions\n",
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def tanh(Z):\n",
    "    A = np.tanh(Z)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def one_hot_encoding(y, dims):\n",
    "    y_encoded = np.zeros((dims[0], dims[1]))\n",
    "    y_encoded[np.arange(dims[0]), y[:,0]] = 1\n",
    "    return y_encoded\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_parameters(layer_dims):\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    num_layers = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, num_layers):\n",
    "        # TODO - this need to be reviewed for random initialisation\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def batch_normalisation():\n",
    "    return None\n",
    "\n",
    "def forward_propagation():\n",
    "    return None\n",
    "\n",
    "def backward_propagation():\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    return None\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "y = train_labels\n",
    "y = one_hot_encoding(train_labels, [train_labels.shape[0], 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 128)\n",
      "(16, 1)\n",
      "(16, 1)\n",
      "0\n",
      "[4.06520116e-01 3.48001378e-08 9.98775638e-01 8.82718624e-01\n",
      " 9.99517239e-01 9.98043736e-01 7.06926682e-01 1.45373404e-04\n",
      " 5.72156255e-03 3.93994881e-01 7.27264438e-02 4.31599949e-04\n",
      " 6.62541231e-06 9.99012693e-01 5.17390863e-03 1.45511410e-01\n",
      " 3.91231469e-02 1.06930322e-01 7.78134421e-01 9.99997913e-01\n",
      " 2.63752697e-02 1.37444804e-03 9.74384135e-01 9.98830941e-01\n",
      " 8.45837379e-03 4.88168771e-01 1.56112048e-04 9.96664960e-01\n",
      " 9.99997890e-01 9.18461856e-01 6.60602130e-03 9.99999489e-01]\n",
      "[4.41869692e-01 3.78262367e-08 1.08562569e+00 9.59476766e-01\n",
      " 1.08643178e+00 1.08483015e+00 7.68398568e-01 1.58014569e-04\n",
      " 6.21908973e-03 4.28255306e-01 7.90504824e-02 4.69130380e-04\n",
      " 7.20153512e-06 1.08588336e+00 5.62381373e-03 1.58164577e-01\n",
      " 4.25251597e-02 1.16228611e-01 8.45798283e-01 1.08695425e+00\n",
      " 2.86687714e-02 1.49396526e-03 1.05911319e+00 1.08568581e+00\n",
      " 9.19388455e-03 5.30618230e-01 1.69687008e-04 1.08333148e+00\n",
      " 1.08695423e+00 9.98328105e-01 0.00000000e+00 1.08695597e+00]\n",
      "(16, 128)\n",
      "(16, 1)\n",
      "(16, 1)\n",
      "1\n",
      "[4.91358817e-02 8.10347457e-01 9.65868514e-01 9.99998948e-01\n",
      " 4.62751600e-02 7.25873190e-01 9.97961647e-01 9.99999713e-01\n",
      " 8.82276598e-01 9.49560076e-06 9.90598655e-01 7.83099246e-02\n",
      " 2.03175600e-01 7.87514978e-01 1.94575029e-02 5.74416770e-01\n",
      " 2.53807599e-01 1.87644923e-01 9.85910902e-01 5.73580649e-01\n",
      " 1.42742372e-02 4.22567692e-01 4.63068426e-03 9.99838508e-01\n",
      " 9.99939893e-01 9.37481266e-01 9.99997549e-01 1.05542969e-01\n",
      " 9.05462142e-01 9.75025492e-01 9.11053051e-05 9.98660039e-01]\n",
      "[5.34085670e-02 8.80812454e-01 1.04985708e+00 1.08695538e+00\n",
      " 5.02990870e-02 7.88992598e-01 1.08474092e+00 1.08695621e+00\n",
      " 9.58996302e-01 1.03213052e-05 1.07673767e+00 0.00000000e+00\n",
      " 0.00000000e+00 8.55994542e-01 2.11494597e-02 6.24366054e-01\n",
      " 2.75877825e-01 2.03961873e-01 1.07164228e+00 6.23457227e-01\n",
      " 1.55154752e-02 4.59312708e-01 5.03335246e-03 1.08678099e+00\n",
      " 1.08689119e+00 1.01900138e+00 1.08695386e+00 0.00000000e+00\n",
      " 9.84197980e-01 1.05981032e+00 9.90275056e-05 1.08550004e+00]\n",
      "(16, 128)\n",
      "(16, 1)\n",
      "(16, 1)\n",
      "2\n",
      "[9.97709499e-01 2.52396509e-06 2.68698938e-03 5.52414776e-01\n",
      " 9.99782173e-01 3.75657220e-01 7.47771785e-02 1.43278765e-03\n",
      " 7.60857770e-02 1.74830448e-02 9.99219300e-01 2.77551210e-03\n",
      " 2.48906778e-02 4.99831108e-05 9.94402474e-01 1.70275782e-04\n",
      " 7.86328684e-01 9.99960066e-01 1.00602165e-04 3.76458614e-01\n",
      " 9.99953459e-01 9.99795574e-01 9.99970443e-01 2.34870166e-01\n",
      " 9.99995692e-01 8.80397389e-01 9.41636152e-01 9.98004632e-01\n",
      " 7.49784481e-01 9.98540691e-01 9.99825545e-01 6.94858913e-02]\n",
      "[0.00000000e+00 0.00000000e+00 2.92064063e-03 6.00450844e-01\n",
      " 1.08671975e+00 0.00000000e+00 8.12795418e-02 1.55737788e-03\n",
      " 8.27019315e-02 1.90033095e-02 1.08610793e+00 3.01686098e-03\n",
      " 2.70550846e-02 5.43294683e-05 1.08087225e+00 1.85082372e-04\n",
      " 8.54705091e-01 1.08691312e+00 1.09350180e-04 4.09194145e-01\n",
      " 1.08690593e+00 1.08673432e+00 1.08692439e+00 2.55293659e-01\n",
      " 1.08695184e+00 9.56953683e-01 1.02351756e+00 1.08478764e+00\n",
      " 8.14983132e-01 1.08537032e+00 1.08676690e+00 7.55281427e-02]\n",
      "(16, 128)\n",
      "(16, 1)\n",
      "(16, 1)\n",
      "3\n",
      "[9.99994654e-01 3.37325357e-01 8.33469136e-01 4.45059438e-01\n",
      " 3.07216669e-06 9.99473071e-01 7.39321203e-01 9.94101176e-01\n",
      " 2.75150024e-01 9.52111325e-01 9.97694078e-01 4.52133121e-04\n",
      " 9.21185654e-01 3.71890540e-01 2.41582612e-01 9.99126970e-01\n",
      " 2.31731414e-03 3.02107910e-02 9.68532308e-01 9.97947643e-01\n",
      " 9.97804466e-01 7.18362276e-01 1.85965349e-06 9.62922609e-01\n",
      " 9.98968667e-01 9.90678247e-01 9.38419880e-01 9.22690508e-01\n",
      " 1.27306323e-01 8.76867608e-01 2.70516678e-06 3.50565675e-01]\n",
      "[1.08695071e+00 3.66657997e-01 9.05944713e-01 4.83760258e-01\n",
      " 3.33931162e-06 1.08638377e+00 0.00000000e+00 1.08054476e+00\n",
      " 2.99076113e-01 1.03490361e+00 1.08445009e+00 4.91449044e-04\n",
      " 1.00128875e+00 4.04228848e-01 0.00000000e+00 1.08600758e+00\n",
      " 2.51881972e-03 3.28378163e-02 1.05275251e+00 1.08472570e+00\n",
      " 0.00000000e+00 7.80828561e-01 2.02136249e-06 1.04665501e+00\n",
      " 1.08583551e+00 0.00000000e+00 1.02002161e+00 1.00292447e+00\n",
      " 1.38376438e-01 9.53116966e-01 2.94039867e-06 3.81049646e-01]\n",
      "(16, 128)\n",
      "(16, 1)\n",
      "(16, 1)\n",
      "4\n",
      "[4.70436243e-01 9.99957699e-01 8.36149087e-01 3.90269217e-02\n",
      " 1.70139334e-04 9.93092352e-01 9.99759183e-01 6.55388715e-01\n",
      " 9.99999782e-01 3.95038287e-07 9.99999319e-01 9.92203078e-01\n",
      " 4.18826889e-01 9.91371754e-01 7.91186447e-01 2.45084886e-04\n",
      " 4.89824701e-01 2.60798979e-03 8.06193954e-01 4.90711473e-06\n",
      " 2.58862345e-01 2.70952024e-05 8.33179256e-03 9.99936834e-01\n",
      " 1.64242526e-02 3.03683955e-03 4.07954091e-01 5.81018590e-01\n",
      " 9.98822460e-01 1.96511613e-04 1.33115880e-06 9.42010934e-01]\n",
      "[5.11343742e-01 1.08691054e+00 9.08857703e-01 4.24205671e-02\n",
      " 1.84934059e-04 1.07944821e+00 1.08669476e+00 7.12379038e-01\n",
      " 1.08695628e+00 4.29389443e-07 1.08695578e+00 1.07848161e+00\n",
      " 4.55246619e-01 1.07757799e+00 8.59985268e-01 2.66396615e-04\n",
      " 5.32418153e-01 2.83477152e-03 8.76297776e-01 5.33382036e-06\n",
      " 2.81372114e-01 2.94513070e-05 9.05629626e-03 1.08688786e+00\n",
      " 1.78524485e-02 3.30091255e-03 4.43428360e-01 0.00000000e+00\n",
      " 1.08567659e+00 2.13599579e-04 1.44691173e-06 1.02392493e+00]\n",
      "(16, 128)\n",
      "(16, 1)\n",
      "(16, 1)\n",
      "5\n",
      "[1.00000000e+00 9.98413419e-01 9.99999957e-01 1.05617020e-03\n",
      " 8.41799796e-06 3.25667253e-02 1.58858862e-05 1.00000000e+00\n",
      " 1.79520767e-19 9.97945588e-01 1.76611592e-06 2.34149576e-06\n",
      " 1.00000000e+00 9.98555578e-01 3.40673871e-07 9.99999998e-01\n",
      " 1.00000000e+00 2.90022531e-01 9.68320164e-01 1.00000000e+00\n",
      " 1.41033498e-02 3.12588961e-03 2.03003102e-14 1.56880829e-11\n",
      " 9.60641296e-05 9.97389062e-01 9.98274996e-01 9.96801172e-01\n",
      " 6.90932294e-05 1.00000000e+00 9.99999950e-01 2.68976976e-02]\n",
      "[1.08695652e+00 1.08523198e+00 1.08695648e+00 1.14801109e-03\n",
      " 9.14999779e-06 3.53986144e-02 1.72672676e-05 1.08695652e+00\n",
      " 1.95131268e-19 1.08472347e+00 1.91969122e-06 2.54510409e-06\n",
      " 1.08695652e+00 1.08538650e+00 3.70297686e-07 1.08695652e+00\n",
      " 1.08695652e+00 3.15241882e-01 1.05252192e+00 1.08695652e+00\n",
      " 1.53297280e-02 3.39770610e-03 2.20655546e-14 1.70522641e-11\n",
      " 1.04417532e-04 1.08411855e+00 1.08508152e+00 1.08347953e+00\n",
      " 7.51013363e-05 1.08695652e+00 1.08695647e+00 2.92366278e-02]\n",
      "(16, 128)\n",
      "(16, 1)\n",
      "(16, 1)\n",
      "6\n",
      "[1.00000000e+000 1.00000000e+000 1.00000000e+000 0.00000000e+000\n",
      " 1.97602837e-168 4.26580418e-066 1.75620390e-287 1.00000000e+000\n",
      " 3.39154155e-078 1.00000000e+000 0.00000000e+000 1.00000000e+000\n",
      " 1.00000000e+000 8.36712861e-201 0.00000000e+000 1.00000000e+000\n",
      " 1.00000000e+000 1.00000000e+000 4.29058724e-059 1.00000000e+000\n",
      " 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      " 0.00000000e+000 1.00000000e+000 1.00000000e+000 6.95216162e-140\n",
      " 0.00000000e+000 1.00000000e+000 0.00000000e+000 0.00000000e+000]\n",
      "[1.08695652e+000 1.08695652e+000 1.08695652e+000 0.00000000e+000\n",
      " 2.14785692e-168 4.63674367e-066 1.90891728e-287 1.08695652e+000\n",
      " 3.68645820e-078 1.08695652e+000 0.00000000e+000 1.08695652e+000\n",
      " 1.08695652e+000 9.09470501e-201 0.00000000e+000 1.08695652e+000\n",
      " 1.08695652e+000 1.08695652e+000 4.66368178e-059 1.08695652e+000\n",
      " 0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      " 0.00000000e+000 1.08695652e+000 1.08695652e+000 7.55669741e-140\n",
      " 0.00000000e+000 1.08695652e+000 0.00000000e+000 0.00000000e+000]\n",
      "(16, 128)\n",
      "(16, 1)\n",
      "(16, 1)\n",
      "7\n",
      "[1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 1. 0. 0.]\n",
      "[1.08695652 1.08695652 1.08695652 0.         0.         0.\n",
      " 0.         1.08695652 0.         1.08695652 0.         1.08695652\n",
      " 1.08695652 0.         0.         1.08695652 1.08695652 1.08695652\n",
      " 0.         1.08695652 0.         0.         0.         0.\n",
      " 0.         1.08695652 1.08695652 0.         0.         1.08695652\n",
      " 0.         0.        ]\n",
      "(16, 128)\n",
      "(16, 1)\n",
      "(16, 1)\n",
      "8\n",
      "[0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 0. 0. 1. 1. 0. 1. 1.]\n",
      "[0.         0.         0.         1.08695652 1.08695652 1.08695652\n",
      " 1.08695652 0.         1.08695652 0.         1.08695652 0.\n",
      " 0.         1.08695652 1.08695652 0.         0.         0.\n",
      " 0.         0.         1.08695652 1.08695652 1.08695652 1.08695652\n",
      " 1.08695652 0.         0.         1.08695652 1.08695652 0.\n",
      " 1.08695652 1.08695652]\n",
      "(16, 128)\n",
      "(16, 1)\n",
      "(16, 1)\n",
      "9\n",
      "[1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 1. 0. 0.]\n",
      "[1.08695652 0.         1.08695652 0.         0.         0.\n",
      " 0.         1.08695652 0.         1.08695652 0.         1.08695652\n",
      " 1.08695652 0.         0.         1.08695652 1.08695652 1.08695652\n",
      " 0.         1.08695652 0.         0.         0.         0.\n",
      " 0.         0.         1.08695652 0.         0.         1.08695652\n",
      " 0.         0.        ]\n",
      "(16, 128)\n",
      "(16, 1)\n",
      "(16, 1)\n",
      "10\n",
      "[0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 0. 0. 1. 1. 0. 1. 1.]\n",
      "[0.         0.         0.         1.08695652 1.08695652 1.08695652\n",
      " 1.08695652 0.         1.08695652 0.         1.08695652 0.\n",
      " 0.         1.08695652 1.08695652 0.         0.         0.\n",
      " 0.         0.         1.08695652 1.08695652 1.08695652 1.08695652\n",
      " 1.08695652 0.         0.         1.08695652 1.08695652 0.\n",
      " 1.08695652 1.08695652]\n",
      "(16, 128)\n",
      "(16, 1)\n",
      "(16, 1)\n",
      "11\n",
      "[1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 1. 0. 0.]\n",
      "[1.08695652 1.08695652 1.08695652 0.         0.         0.\n",
      " 0.         1.08695652 0.         1.08695652 0.         1.08695652\n",
      " 1.08695652 0.         0.         1.08695652 1.08695652 1.08695652\n",
      " 1.08695652 1.08695652 0.         0.         0.         0.\n",
      " 0.         1.08695652 1.08695652 0.         0.         1.08695652\n",
      " 0.         0.        ]\n",
      "(16, 128)\n",
      "(16, 1)\n",
      "(16, 1)\n",
      "12\n",
      "[0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 0. 0. 1. 1. 0. 1. 1.]\n",
      "[0.         0.         0.         1.08695652 1.08695652 1.08695652\n",
      " 1.08695652 0.         1.08695652 0.         1.08695652 0.\n",
      " 0.         1.08695652 1.08695652 0.         0.         0.\n",
      " 0.         0.         1.08695652 1.08695652 1.08695652 1.08695652\n",
      " 1.08695652 0.         0.         1.08695652 1.08695652 0.\n",
      " 1.08695652 1.08695652]\n",
      "(16, 128)\n",
      "(16, 1)\n",
      "(16, 1)\n",
      "13\n",
      "[1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 1. 0. 0.]\n",
      "[1.08695652 1.08695652 0.         0.         0.         0.\n",
      " 0.         1.08695652 0.         1.08695652 0.         1.08695652\n",
      " 1.08695652 0.         0.         1.08695652 1.08695652 1.08695652\n",
      " 1.08695652 1.08695652 0.         0.         0.         0.\n",
      " 0.         1.08695652 1.08695652 0.         0.         1.08695652\n",
      " 0.         0.        ]\n",
      "(16, 128)\n",
      "(16, 1)\n",
      "(16, 1)\n",
      "14\n",
      "[0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 0. 0. 1. 1. 0. 1. 1.]\n",
      "[0.         0.         0.         1.08695652 1.08695652 1.08695652\n",
      " 1.08695652 0.         1.08695652 0.         0.         0.\n",
      " 0.         1.08695652 1.08695652 0.         0.         0.\n",
      " 0.         0.         1.08695652 1.08695652 1.08695652 0.\n",
      " 1.08695652 0.         0.         1.08695652 1.08695652 0.\n",
      " 1.08695652 1.08695652]\n",
      "(16, 128)\n",
      "(16, 1)\n",
      "(16, 1)\n",
      "15\n",
      "[1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 1. 0. 0.]\n",
      "[1.08695652 1.08695652 1.08695652 0.         0.         0.\n",
      " 0.         1.08695652 0.         1.08695652 0.         1.08695652\n",
      " 1.08695652 0.         0.         1.08695652 1.08695652 1.08695652\n",
      " 1.08695652 1.08695652 0.         0.         0.         0.\n",
      " 0.         1.08695652 1.08695652 0.         0.         1.08695652\n",
      " 0.         0.        ]\n",
      "(16, 128)\n",
      "(16, 1)\n",
      "(16, 1)\n",
      "16\n",
      "[0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 0. 0. 1. 1. 0. 1. 1.]\n",
      "[0.         0.         0.         1.08695652 1.08695652 1.08695652\n",
      " 1.08695652 0.         1.08695652 0.         1.08695652 0.\n",
      " 0.         1.08695652 1.08695652 0.         0.         0.\n",
      " 0.         0.         1.08695652 1.08695652 1.08695652 1.08695652\n",
      " 1.08695652 0.         0.         1.08695652 1.08695652 0.\n",
      " 1.08695652 0.        ]\n",
      "(16, 128)\n",
      "(16, 1)\n",
      "(16, 1)\n",
      "17\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: RuntimeWarning: overflow encountered in exp\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:45: RuntimeWarning: overflow encountered in matmul\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:79: RuntimeWarning: overflow encountered in matmul\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:87: RuntimeWarning: overflow encountered in matmul\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:88: RuntimeWarning: invalid value encountered in add\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:89: RuntimeWarning: invalid value encountered in add\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:60: RuntimeWarning: invalid value encountered in greater\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:71: RuntimeWarning: invalid value encountered in greater\n"
     ]
    }
   ],
   "source": [
    "x = train_data\n",
    "y = one_hot_encoding(train_labels, [train_labels.shape[0], 10])\n",
    "xtest = validation_data\n",
    "ytest = validation_labels\n",
    "\n",
    "np.random.seed(3)\n",
    "\n",
    "loss = np.arange(1, max_iteration)\n",
    "w1_new = np.zeros((hidden_layer_dim, train_data.shape[1] + 1))\n",
    "w2_new = np.zeros((hidden_layer_dim, hidden_layer_dim + 1))\n",
    "w3_new = np.zeros((output_layer_dim, hidden_layer_dim + 1))\n",
    "w1 = 2 * np.random.rand(hidden_layer_dim, train_data.shape[1] + 1) - 1\n",
    "w2 = 2 * np.random.rand(hidden_layer_dim, hidden_layer_dim + 1) - 1\n",
    "w3 = 2 * np.random.rand(output_layer_dim, hidden_layer_dim + 1) - 1\n",
    "momentum1 = 0\n",
    "momentum2 = 0\n",
    "momentum3 = 0\n",
    "\n",
    "trainsize = 50048\n",
    "js = int(trainsize / batch_size)\n",
    "# 16 x 128 matrix\n",
    "p = np.random.randint(0, trainsize, size=trainsize).reshape((batch_size, js))\n",
    "gamma1=1;\n",
    "beta1=0;\n",
    "gamma2=1;\n",
    "beta2=0;\n",
    "\n",
    "\n",
    "for iteration in np.arange(0, max_iteration):\n",
    "    for j in np.arange(0, js):\n",
    "        rate_drop=0.92\n",
    "        \n",
    "        xtemp = x[p[:,j],:]\n",
    "#       batch normalisation\n",
    "        xbar = np.mean(xtemp, axis=1).reshape(xtemp.shape[0], 1)\n",
    "        xvar = np.var(xtemp.T, axis=0).T.reshape(xtemp.shape[0], 1)\n",
    "        xtemp = np.divide((xtemp - xbar), np.sqrt(xvar + 1e-8))\n",
    "        xtemp1 = np.concatenate((np.ones((batch_size,1)), gamma1*xtemp+beta1), axis=1)\n",
    "\n",
    "        \n",
    "#       calculate hidden layer\n",
    "        z1 = 1 / (1 + np.exp(- np.matmul(w1, xtemp1.T))).T\n",
    "#       cauculate output layer\n",
    "#       z1 = 1 ./ (1 + exp(-w2 * [ones(1,numTP); z2']))';\n",
    "        drop = np.random.rand(batch_size, hidden_layer_dim) < rate_drop\n",
    "        z1 = z1 * drop / rate_drop\n",
    "        print(z1[0,:])\n",
    "        \n",
    "#         %zbar = mean(z1,2);        \n",
    "#         %zvar = var(z1')';\n",
    "#         %zbar = (zbar - zbar)./sqrt(zvar+1e-8);\n",
    "        \n",
    "        z11 = np.concatenate((np.ones((batch_size,1)), gamma2*z1+beta2), axis=1)           \n",
    "        z2 = np.matmul(w2, z11.T)\n",
    "        z2 = z2 * (z2 > 0)\n",
    "        z2 = z2.T\n",
    "        \n",
    "        \n",
    "        z3 = np.matmul(w3, np.concatenate((np.ones((1, batch_size)), z2.T), axis=0))\n",
    "        z3 = z3.T\n",
    "        z3 = softmax(z3.T).T\n",
    "#       calculate gradient output layer\n",
    "#       dz2/d(w2*z1)\n",
    "        delta3 = y[p[:,j],:] - z3\n",
    "#       calculate gragient hidden layer\n",
    "        delta2 = np.matmul(delta3, w3[:,1:]) * ( z2 > 0 )\n",
    "        \n",
    "#       calculate gragient hidden layer\n",
    "#       dz2/d(w1*xtemp1)\n",
    "        delta1 = z1 * (1 - z1) * drop * (np.matmul(delta2, w2[:,1:])) / rate_drop\n",
    "#       delta1 = delta1.*(delta1>0);\n",
    "        change3 = np.matmul(delta3.T, np.concatenate((np.ones((batch_size,1)), z2), axis=1)) / batch_size\n",
    "        change2 = np.matmul(delta2.T, np.concatenate((np.ones((batch_size,1)), z1), axis=1)) / batch_size\n",
    "        change1 = np.matmul(delta1.T, xtemp1) / batch_size\n",
    "#       sum of training pattern\n",
    "        w3_new = learning_rate * (change3 - 0.001*w3+0.5*momentum3)\n",
    "        w2_new = learning_rate * (change2 - 0.001*w2+0.5*momentum2)\n",
    "        w1_new = learning_rate * (change1 - 0.001*w1+0.5*momentum1)\n",
    "        momentum3 = change3\n",
    "        momentum2 = change2\n",
    "        momentum1 = change1\n",
    "        dbeta = np.matmul(delta1, w1[:,1:])\n",
    "        dgamma = sum(dbeta * xtemp) / batch_size\n",
    "        dbeta = sum(dbeta) / batch_size\n",
    "        gamma1 = gamma1 + 0.0005*dgamma\n",
    "        beta1 = beta1 + 0.0005*dbeta\n",
    "        dbeta = np.matmul(delta2, w2[:,1:])\n",
    "        dgamma = sum(dbeta * z1) / batch_size\n",
    "        dbeta = sum(dbeta) / batch_size\n",
    "        gamma2 = gamma2 + 0.0005*dgamma\n",
    "        beta2 = beta2 + 0.0005*dbeta\n",
    "#       update w2\n",
    "        w3 = w3 + w3_new\n",
    "        w2 = w2 + w2_new\n",
    "#       update w1\n",
    "        w1 = w1 + w1_new\n",
    "        if math.isnan(w1[0,0]):\n",
    "#             print(j)\n",
    "#             print(w1_new)\n",
    "#             print(delta1)\n",
    "#             print(delta2)\n",
    "#             print(delta3)\n",
    "#             print(z2)\n",
    "#             print(z1)\n",
    "            break   \n",
    "    break\n",
    "#     mean square error\n",
    "#     mse(1,iteration) = sum(sum((o-t).^2)')/(numTP*numOut)\n",
    "#     [~,i]=max(a,[],2);\n",
    "#     \n",
    "#     loss(iteration)\n",
    "    print(f'iteration: {iteration}')\n",
    "#     plot map and decision boundary\n",
    "#     calculate hidden layer\n",
    "    xbar = np.mean(xtest, axis=1).reshape(xtest.shape[0], 1)     \n",
    "    xvar = np.var(xtest.T, axis=0).T.reshape(xtest.shape[0], 1) \n",
    "    xtest1 = np.divide((xtest - xbar), np.sqrt(xvar+1e-8))\n",
    "    xtest2 = np.concatenate((np.ones((xtest.shape[0],1)), gamma1*xtest1+beta1), axis=1)\n",
    "    z1 = 1 / (1 + np.exp(-np.matmul(w1, xtest2.T))).T\n",
    "    \n",
    "#     %zbar = mean(z1,2);        \n",
    "#     %zvar = var(z1')';\n",
    "#     %zbar = (zbar - zbar)./sqrt(zvar+1e-8);\n",
    "    \n",
    "    z11 = np.concatenate((np.ones((xtest.shape[0],1)), gamma2*z1+beta2), axis=1)   \n",
    "    z2 = np.matmul(w2, z11.T)\n",
    "    z2 = z2 * (z2>0)    \n",
    "    z2 = z2.T\n",
    "#     % cauculate output layer\n",
    "#     %z1 = 1 ./ (1 + exp(-w2 * [ones(1,numTP); z2']))';\n",
    "    nn = z1.shape[0]\n",
    "    z3 = np.matmul(w3, np.concatenate((np.ones((1,nn)), z2.T), axis=0))\n",
    "    z3 = z3.T\n",
    "#     %for i=1:9\n",
    "#     %    o = 1 ./ (1 + exp(-w2 * [ones(1,numTP); z']))';\n",
    "#     %end\n",
    "    z3 = softmax(z3.T).T \n",
    "#     [~,i]=max(z3,[],2);\n",
    "    res = np.amax(z3, axis=1)\n",
    "\n",
    "    accuracy = np.sum(res==ytest) / nn * 100\n",
    "    loss[iteration] = accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9952, 9952)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res == ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
