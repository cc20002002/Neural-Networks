\section{Conclusion}
In conclusion, our numerical simulation supports the published results on various state-of-the-arts modules. We find BN is the most significant module to improve CV accuracy and predictive capability. In terms of speed, mini-batch training is the most important module follow by the momentum method. However, the size of the batch as well as the momentum coefficients need to be carefully tuned. Also, simulation results shows that dropout is useful for large neural networks where the number of parameters is large comparing with the number of samples. We use shell script to train neural network models with different hyperparameters in a parallel setting. Parallel computing fully utilises our computing resources and double the speed to train ten neural network models.

In recent years, \textsc{nvidia} released their \href{https://developer.nvidia.com/cuda-zone}{\texttt{Cuda}} package which parallelises matrix multiplications using \textsc{gpu}. Large and iterative matrix multiplications is the most computational intensive part when training neural network, This package improves the speed of matrix multiplication by a factor of $>20$ \citep{5452452}. As a computationally expensive procedure, a natural extension of this project is to implement \href{https://developer.nvidia.com/cuda-zone}{\texttt{Cuda}} to speed up the training speed of our neural network.  