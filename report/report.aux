\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\zref@newlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{unsrtnat}
\citation{lee2001algorithms}
\babel@aux{UKenglish}{}
\citation{lee1999learning}
\citation{guillamet2002non}
\citation{lee1999learning}
\newlabel{chapter1}{{1}{2}{Introduction\label {chapter1}}{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}}
\citation{lee2001algorithms}
\citation{lee2001algorithms}
\citation{lee2001algorithms}
\citation{berry2007algorithms}
\citation{guan2017truncated}
\citation{yang2011kullback}
\citation{lee2001algorithms}
\citation{lin2007convergence}
\citation{guan2012nenmf}
\citation{berry2007algorithms}
\citation{guan2012nenmf}
\citation{kim2008nonnegative}
\citation{kim2008nonnegative}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related work}{3}{section.2}}
\newlabel{chp3}{{2}{3}{Related work}{section.2}{}}
\citation{lam2008non}
\citation{kong2011robust}
\citation{guan2017truncated}
\citation{barbu2013variational}
\citation{lee2001algorithms}
\citation{lee2001algorithms}
\citation{liu2015performance}
\citation{lee2001algorithms}
\newlabel{chapter2}{{3}{4}{Methods \label {chapter2}}{section.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methods }{4}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}NMF and Gaussian noise}{4}{subsection.3.1}}
\newlabel{eq:obnmf}{{1}{4}{NMF and Gaussian noise}{equation.3.1}{}}
\newlabel{eq:nmf}{{2}{4}{NMF and Gaussian noise}{equation.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}KLNMF and Poisson noise}{4}{subsection.3.2}}
\citation{lee2001algorithms}
\citation{scaless}
\newlabel{eq:klobj}{{3}{5}{KLNMF and Poisson noise}{equation.3.3}{}}
\newlabel{eq:klnmf}{{4}{5}{KLNMF and Poisson noise}{equation.3.4}{}}
\citation{Walck:1996cca}
\citation{sampat2005computer}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Preprocess}{6}{subsection.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Gaussian and Poisson are asymptotic equivalent}{6}{subsection.3.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Salt \& Pepper noise}{6}{subsection.3.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Multiple initial estimates assure the algorithms stable}{6}{subsection.3.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Compare a Gaussian noise~$N(0,40)$ with Poisson noise $\operatorname  {Poi}(40)-40$. They two distributions are asymptotically equivalent and have overlapped density functions. The Gaussian noise~$N(0,80^2)$ is very different.}}{7}{figure.1}}
\newlabel{noise}{{1}{7}{Compare a Gaussian noise~$N(0,40)$ with Poisson noise $\operatorname {Poi}(40)-40$. They two distributions are asymptotically equivalent and have overlapped density functions. The Gaussian noise~$N(0,80^2)$ is very different}{figure.1}{}}
\newlabel{matn1}{{1}{7}{Multi-start paralle computing}{lstlisting.1}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {1}Multi-start paralle computing}{7}{lstlisting.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Residual of the objective function~\textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:obnmf}\unskip \@@italiccorr )}} and~\textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:klobj}\unskip \@@italiccorr )}} versus the number of iterations. \textsc  {nmf} converges more than twice faster comparing with \textsc  {klnmf}.}}{8}{figure.2}}
\newlabel{error}{{2}{8}{Residual of the objective function~\eqref {eq:obnmf} and~\eqref {eq:klobj} versus the number of iterations. \textsc {nmf} converges more than twice faster comparing with \textsc {klnmf}}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}KLNMF requires more iterations}{8}{subsection.3.7}}
\citation{Walck:1996cca}
\newlabel{ci}{{3.8}{9}{Evaluation metrics and their confidence intervals \label {ci}}{subsection.3.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8}Evaluation metrics and their confidence intervals }{9}{subsection.3.8}}
\newlabel{eq:boot}{{5}{9}{Evaluation metrics and their confidence intervals \label {ci}}{equation.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9}Statistical method compares the robustness of algorithms}{9}{subsection.3.9}}
\newlabel{epdf}{{6}{9}{Statistical method compares the robustness of algorithms}{equation.3.6}{}}
\newlabel{teststatistic}{{7}{9}{Statistical method compares the robustness of algorithms}{equation.3.7}{}}
\citation{belhumeur1997eigenfaces}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{10}{section.4}}
\newlabel{chapter4}{{4}{10}{Experiments}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Dataset}{10}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Noise}{10}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Gaussian Noise}{10}{subsubsection.4.2.1}}
\newlabel{sec:gau}{{4.2.1}{10}{Gaussian Noise}{subsubsection.4.2.1}{}}
\newlabel{gau}{{2}{10}{Gaussian Noise Design}{lstlisting.2}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2}Gaussian Noise Design}{10}{lstlisting.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Poisson Noise}{11}{subsubsection.4.2.2}}
\newlabel{sec:poi}{{4.2.2}{11}{Poisson Noise}{subsubsection.4.2.2}{}}
\newlabel{poi}{{3}{11}{Poisson Noise Design}{lstlisting.3}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3}Poisson Noise Design}{11}{lstlisting.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Salt \& Pepper Noise}{11}{subsubsection.4.2.3}}
\newlabel{sec:sal}{{4.2.3}{11}{Salt \& Pepper Noise}{subsubsection.4.2.3}{}}
\newlabel{salt}{{4}{11}{Salt and Pepper Noise Design}{lstlisting.4}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4}Salt and Pepper Noise Design}{11}{lstlisting.4}}
\citation{guan2017truncated}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Experiment Setup}{12}{subsection.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Experiments Results}{12}{subsection.4.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Two algorithms output similar reconstructed images}{12}{subsubsection.4.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Hypothesis test distinguishes the difference in RRE}{12}{subsubsection.4.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The reconstructed image by \textsc  {nmf}. The original images (Column~1) are combined with noises (Column~1) including Gaussian Noise with Variance~$80$ (Row~2), Poisson Noise (Row~3), and Salt \& Pepper Noise (Row~4). The corrupted images are shown in Column~3. The reconstructed images are shown in (Column~4). The reconstruction with no noise is shown in Row~1.}}{13}{figure.3}}
\newlabel{noisesnmff}{{3}{13}{The reconstructed image by \textsc {nmf}. The original images (Column~1) are combined with noises (Column~1) including Gaussian Noise with Variance~$80$ (Row~2), Poisson Noise (Row~3), and Salt \& Pepper Noise (Row~4). The corrupted images are shown in Column~3. The reconstructed images are shown in (Column~4). The reconstruction with no noise is shown in Row~1}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The reconstructed image by \textsc  {klnmf}. The original images (Column~1) are combined with noises (Column~1) including Gaussian Noise with Variance~$80$ (Row~2), Poisson Noise (Row~3), and Salt \& Pepper Noise (Row~4). The corrupted images are shown in Column~3. The reconstructed images are shown in (Column~4). The reconstruction with no noise is shown in Row~1.}}{14}{figure.4}}
\newlabel{noisesklnmff}{{4}{14}{The reconstructed image by \textsc {klnmf}. The original images (Column~1) are combined with noises (Column~1) including Gaussian Noise with Variance~$80$ (Row~2), Poisson Noise (Row~3), and Salt \& Pepper Noise (Row~4). The corrupted images are shown in Column~3. The reconstructed images are shown in (Column~4). The reconstruction with no noise is shown in Row~1}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Histogram of \textsc  {rre} results from 80 Monte-Carlo simulations. Blue bars correspond to \textsc  {nmf}, and the pink bars correspond to \textsc  {klnmf}. The types of noise are labelled in the plot. The visualisation agrees with our statistical analysis.}}{15}{figure.5}}
\newlabel{histo}{{5}{15}{Histogram of \textsc {rre} results from 80 Monte-Carlo simulations. Blue bars correspond to \textsc {nmf}, and the pink bars correspond to \textsc {klnmf}. The types of noise are labelled in the plot. The visualisation agrees with our statistical analysis}{figure.5}{}}
\citation{guan2017truncated}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Average of evaluations metrics over $80$ Monte-Carlo simulations using the \texttt  {ORL} dataset. The 95\% confidence intervals are calculated using bootstrap.}}{16}{table.1}}
\newlabel{tab:ci}{{1}{16}{Average of evaluations metrics over $80$ Monte-Carlo simulations using the \texttt {ORL} dataset. The 95\% confidence intervals are calculated using bootstrap}{table.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.3}ACC and NMI results}{16}{subsubsection.4.4.3}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Average of evaluations metrics over 10 Monte-Carlo simulations using \texttt  {CroppedYale} dataset.}}{17}{table.2}}
\newlabel{tb:yale}{{2}{17}{Average of evaluations metrics over 10 Monte-Carlo simulations using \texttt {CroppedYale} dataset}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Personal Reflection}{17}{subsection.4.5}}
\bibdata{research}
\bibcite{lee2001algorithms}{{1}{2001}{{Lee and Seung}}{{}}}
\bibcite{lee1999learning}{{2}{1999}{{Lee and Seung}}{{}}}
\bibcite{guillamet2002non}{{3}{2002}{{Guillamet and Vitri{\`a}}}{{}}}
\bibcite{berry2007algorithms}{{4}{2007}{{Berry et~al.}}{{Berry, Browne, Langville, Pauca, and Plemmons}}}
\bibcite{guan2017truncated}{{5}{2017}{{Guan et~al.}}{{Guan, Liu, Zhang, Tao, and Davis}}}
\bibcite{yang2011kullback}{{6}{2011}{{Yang et~al.}}{{Yang, Zhang, Yuan, and Oja}}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{18}{section.5}}
\bibcite{lin2007convergence}{{7}{2007}{{Lin}}{{}}}
\bibcite{guan2012nenmf}{{8}{2012}{{Guan et~al.}}{{Guan, Tao, Luo, and Yuan}}}
\bibcite{kim2008nonnegative}{{9}{2008}{{Kim and Park}}{{}}}
\bibcite{lam2008non}{{10}{2008}{{Lam}}{{}}}
\bibcite{kong2011robust}{{11}{2011}{{Kong et~al.}}{{Kong, Ding, and Huang}}}
\bibcite{barbu2013variational}{{12}{2013}{{Barbu}}{{}}}
\bibcite{liu2015performance}{{13}{2016}{{Liu and Tao}}{{}}}
\bibcite{scaless}{{14}{2011}{{Fevotte and Idier}}{{}}}
\bibcite{Walck:1996cca}{{15}{1996}{{Walck}}{{}}}
\bibcite{sampat2005computer}{{16}{2005}{{Sampat et~al.}}{{Sampat, Markey, Bovik, et~al.}}}
\bibcite{belhumeur1997eigenfaces}{{17}{1997}{{Belhumeur et~al.}}{{Belhumeur, Hespanha, and Kriegman}}}
\ulp@afterend
