\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\zref@newlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{unsrtnat}
\citation{lee2001algorithms}
\babel@aux{UKenglish}{}
\newlabel{chapter1}{{1}{2}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}}
\citation{DeepAI}
\citation{lecun1998Gradient}
\newlabel{chapter2}{{2}{3}{Methods}{section.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methods }{3}{section.2}}
\citation{doi:10.1080/00401706.1970.10488634}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Cross validation score and early stopping prevents overfit}{4}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Cross-entropy with weight decay improves the model robustness}{4}{subsection.2.2}}
\newlabel{eq:loss}{{1}{4}{Cross-entropy with weight decay improves the model robustness}{equation.2.1}{}}
\citation{pmlr-v9-glorot10a}
\citation{pmlr-v9-glorot10a}
\citation{pmlr-v9-glorot10a}
\newlabel{eq:loss_reg}{{2}{5}{Cross-entropy with weight decay improves the model robustness}{equation.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Xavier initialisation speeds up convergence}{5}{subsection.2.3}}
\citation{pmlr-v37-ioffe15}
\citation{NIPS20187515}
\citation{DBLP:journals/corr/abs-1207-0580}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Batch normalisation prevents internal covariance shift}{6}{subsection.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Drop out performs model averaging}{6}{subsection.2.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Activation functions map input to desired output range}{6}{subsection.2.6}}
\citation{NIPS20145267}
\newlabel{sec:nonl}{{2.6.1}{7}{Nonlinear activation}{subsubsection.2.6.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.1}Nonlinear activation}{7}{subsubsection.2.6.1}}
\newlabel{eq:tanh}{{4}{7}{Nonlinear activation}{equation.2.4}{}}
\newlabel{eq:sig}{{5}{7}{Nonlinear activation}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2}ReLU activation function}{7}{subsubsection.2.6.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.3}Softmax activation function}{7}{subsubsection.2.6.3}}
\citation{DBLP:journals/corr/GoyalDGNWKTJH17}
\citation{rumelhart1986learning}
\citation{rumelhart1986learning}
\citation{belhumeur1997eigenfaces}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Stochastic gradient descent finds the optimal weights}{8}{subsection.2.7}}
\newlabel{sec:minibatch}{{2.7.1}{8}{Mini-batch improves the speed of computation}{subsubsection.2.7.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.1}Mini-batch improves the speed of computation }{8}{subsubsection.2.7.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.2}Momentum accelerates rate of convergence}{8}{subsubsection.2.7.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.3}todo: adam}{8}{subsubsection.2.7.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experiments and results}{8}{section.3}}
\newlabel{chapter4}{{3}{8}{Experiments and results}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Dataset}{9}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Experiment Setup}{9}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}multiprocessing speed up hyperparameter tuning}{9}{subsubsection.3.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Hardware and software}{9}{subsubsection.3.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Early stopping decides maximum number of iterations}{9}{subsubsection.3.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Experiments Results}{9}{subsection.3.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Optimal hyperparameters}{9}{subsubsection.3.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Put accuracy vs iteration here with the best possible hyperparameter, and parameters without each of normalisation, dropout, momentum}}{10}{figure.1}}
\newlabel{noisesklnmff}{{1}{10}{Put accuracy vs iteration here with the best possible hyperparameter, and parameters without each of normalisation, dropout, momentum}{figure.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Results and parameters of the best four setups.}}{10}{table.1}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Results and parameters of different setups for 89.87 results.}}{10}{table.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Results and parameters of different setups for 89.87 results.}}{11}{figure.2}}
\newlabel{fig:my-label}{{2}{11}{Results and parameters of different setups for 89.87 results}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Experiments Results}{11}{subsection.3.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Vectorisation dramatically improves the speed of training}{11}{subsubsection.3.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Batch normalisation significantly improves the accuracy}{11}{subsubsection.3.4.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.3}Weight decaying prevents overfitting}{11}{subsubsection.3.4.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.4}dropout does not help here}{11}{subsubsection.3.4.4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{11}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Why relu helpful}{11}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Why batch normalisation helpful}{11}{subsection.4.2}}
\bibdata{research}
\bibcite{DeepAI}{{1}{1999}{{Dee}}{{}}}
\bibcite{lecun1998Gradient}{{2}{1998}{{Lecun et~al.}}{{Lecun, Bottou, Bengio, and Haffner}}}
\bibcite{doi:10.1080/00401706.1970.10488634}{{3}{1970}{{Hoerl and Kennard}}{{}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Weight decaying}{12}{subsection.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Why drop out is not helpful}{12}{subsection.4.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Why early stopping}{12}{subsection.4.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Why momentum}{12}{subsection.4.6}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{12}{section.5}}
\bibcite{pmlr-v9-glorot10a}{{4}{2010}{{Glorot and Bengio}}{{}}}
\bibcite{pmlr-v37-ioffe15}{{5}{2015}{{Ioffe and Szegedy}}{{}}}
\bibcite{NIPS20187515}{{6}{2018}{{Santurkar et~al.}}{{Santurkar, Tsipras, Ilyas, and Madry}}}
\bibcite{DBLP:journals/corr/abs-1207-0580}{{7}{2012}{{Hinton et~al.}}{{Hinton, Srivastava, Krizhevsky, Sutskever, and Salakhutdinov}}}
\bibcite{NIPS20145267}{{8}{2014}{{Livni et~al.}}{{Livni, Shalev-Shwartz, and Shamir}}}
\bibcite{DBLP:journals/corr/GoyalDGNWKTJH17}{{9}{2017}{{Goyal et~al.}}{{Goyal, Doll{\'{a}}r, Girshick, Noordhuis, Wesolowski, Kyrola, Tulloch, Jia, and He}}}
\bibcite{rumelhart1986learning}{{10}{1986}{{Rumelhart et~al.}}{{Rumelhart, Hinton, and Williams}}}
\ulp@afterend
