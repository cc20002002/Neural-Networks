\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\zref@newlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{unsrtnat}
\citation{pmlr-v9-glorot10a}
\citation{relu-hahnloser}
\citation{DBLP:journals/corr/abs-1207-0580}
\citation{rumelhart1986learning}
\citation{NIPS1991563}
\citation{DBLP:journals/corr/GoyalDGNWKTJH17}
\citation{pmlr-v37-ioffe15}
\select@language{UKenglish}
\@writefile{toc}{\select@language{UKenglish}}
\@writefile{lof}{\select@language{UKenglish}}
\@writefile{lot}{\select@language{UKenglish}}
\newlabel{chapter1}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{10.2307/4299364}
\citation{rosenblatt}
\citation{58337}
\citation{NIPS1989293}
\citation{Azoff:1994:NNT:561452,KAASTRA1996215}
\citation{554195,655647}
\citation{Collobert:2008:UAN:1390156.1390177,Collobert:2011:NLP:1953048.2078186}
\citation{Bishop:2006:PRM:1162264}
\citation{5725236}
\citation{5725236}
\citation{Bishop:2006:PRM:1162264}
\citation{pmlr-v9-glorot10a}
\citation{pmlr-v9-glorot10a}
\newlabel{chapter2}{{2}{3}{Methods}{section.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methods }{3}{section.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A diagram illustrates the architecture of our benchmark neural network model. The weights of the densely connected layers are initialised by a uniform distribution proposed by \citet  {pmlr-v9-glorot10a}.}}{4}{figure.1}}
\newlabel{fig:nn}{{1}{4}{A diagram illustrates the architecture of our benchmark neural network model. The weights of the densely connected layers are initialised by a uniform distribution proposed by \citet {pmlr-v9-glorot10a}}{figure.1}{}}
\newlabel{sec:early}{{2.1}{4}{Cross validation score and early stopping prevents overfit}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Cross validation score and early stopping prevents overfit}{4}{subsection.2.1}}
\citation{NIPS1991563}
\citation{doi:10.1080/00401706.1970.10488634}
\citation{pmlr-v9-glorot10a}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Cross-entropy with weight decay improves the model robustness}{5}{subsection.2.2}}
\newlabel{eq:loss}{{1}{5}{Cross-entropy with weight decay improves the model robustness}{equation.2.1}{}}
\newlabel{eq:loss_reg}{{2}{5}{Cross-entropy with weight decay improves the model robustness}{equation.2.2}{}}
\newlabel{sec:xav}{{2.3}{5}{Xavier initialisation speeds up convergence}{subsection.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Xavier initialisation speeds up convergence}{5}{subsection.2.3}}
\citation{pmlr-v9-glorot10a}
\citation{pmlr-v9-glorot10a}
\citation{pmlr-v37-ioffe15}
\citation{NIPS20187515}
\citation{dropout}
\citation{DBLP:journals/corr/abs-1207-0580}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}BN prevents internal covariance shift}{6}{subsection.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Dropout performs model averaging}{6}{subsection.2.5}}
\citation{NIPS20145267}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Activation functions map input to desired output range}{7}{subsection.2.6}}
\newlabel{sec:nonl}{{2.6.1}{7}{Nonlinear activation}{subsubsection.2.6.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.1}Nonlinear activation}{7}{subsubsection.2.6.1}}
\newlabel{eq:tanh}{{4}{7}{Nonlinear activation}{equation.2.4}{}}
\newlabel{eq:sig}{{5}{7}{Nonlinear activation}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2}ReLU activation function}{7}{subsubsection.2.6.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.3}Softmax activation function}{7}{subsubsection.2.6.3}}
\citation{DBLP:journals/corr/GoyalDGNWKTJH17}
\citation{rumelhart1986learning}
\citation{rumelhart1986learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Stochastic gradient descent finds the optimal weights}{8}{subsection.2.7}}
\newlabel{sec:minibatch}{{2.7.1}{8}{Mini-batch improves the speed of computation}{subsubsection.2.7.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.1}Mini-batch improves the speed of computation }{8}{subsubsection.2.7.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.2}Momentum accelerates rate of convergence}{8}{subsubsection.2.7.2}}
