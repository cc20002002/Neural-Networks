\BOOKMARK [1][]{section.1}{Introduction}{}% 1
\BOOKMARK [1][]{section.2}{Methods }{}% 2
\BOOKMARK [2][]{subsection.2.1}{Cross validation score and early stopping prevents overfit}{section.2}% 3
\BOOKMARK [2][]{subsection.2.2}{Cross-entropy with weight decay improves the model robustness}{section.2}% 4
\BOOKMARK [2][]{subsection.2.3}{Xavier initialisation speeds up convergence}{section.2}% 5
\BOOKMARK [2][]{subsection.2.4}{BN prevents internal covariance shift}{section.2}% 6
\BOOKMARK [2][]{subsection.2.5}{Dropout performs model averaging}{section.2}% 7
\BOOKMARK [2][]{subsection.2.6}{Activation functions map input to desired output range}{section.2}% 8
\BOOKMARK [3][]{subsubsection.2.6.1}{Nonlinear activation}{subsection.2.6}% 9
\BOOKMARK [3][]{subsubsection.2.6.2}{ReLU activation function}{subsection.2.6}% 10
\BOOKMARK [3][]{subsubsection.2.6.3}{Softmax activation function}{subsection.2.6}% 11
\BOOKMARK [2][]{subsection.2.7}{Stochastic gradient descent finds the optimal weights}{section.2}% 12
\BOOKMARK [3][]{subsubsection.2.7.1}{Mini-batch improves the speed of computation }{subsection.2.7}% 13
\BOOKMARK [3][]{subsubsection.2.7.2}{Momentum accelerates rate of convergence}{subsection.2.7}% 14
