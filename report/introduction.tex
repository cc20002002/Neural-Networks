%check dropout, not drop out
\section{Introduction\label{chapter1}}
%Briefly introduce \textsc{nmf}, applications

%- Whatâ€™s the aim of the study? 
% Draft Completed - Pending Review
This study builds a multi-layer neural network with cross-entropy loss to perform a ten-class classification task. We implement various state-of-the-art neural network modules to improve the test  accuracy  and convergence rate classifying the given dataset.
%The task provides a labelled training dataset and an unlabelled testing dataset.
%The training dataset given for this task consists of $128$ features for $60,000$ samples,
%the test dataset contains $10,000$ samples with same amount of features as training dataset.
%Compare to a simple multi-layer neural network implementation,% @Chen review: I don't understand this sentence
%our neural network includes various state-of-the-art modules to further improve the prediction accuracy and convergence rate. 
The applied modules consist of Xavier initialisation \citep{pmlr-v9-glorot10a}, 
rectified linear unit (ReLU) \citep{relu-hahnloser}, 
dropout \citep{DBLP:journals/corr/abs-1207-0580},
momentum method \citep{rumelhart1986learning},
weight decay \citep{NIPS1991563}, 
mini-batch training \citep{DBLP:journals/corr/GoyalDGNWKTJH17}, 
and batch normalisation \citep{pmlr-v37-ioffe15}.
We numerically validates the performance of these modules by the ten-class classification problem. 
With these modules, our model accurately classifies the cross validation (CV) dataset with an accuracy of $89.9\%$ within $2.3$ minutes. 
To illustrate the effects of each module, our numerical experiment gradually turns the modules off one by one, and then explore the numerical consequences by comparing them with the original benchmark model.

Neural networks were inspired by the biological neural networks which constitute human brains \citep{10.2307/4299364}.
The concept of perception was originally introduced as a probabilistic model for image recongition \citep{rosenblatt}.
%However, \citet{minsky69perceptrons} acknowledges the perception's major limitations due to some mathematical predicates in 1969 and lead pessimism on the machine learning effectiveness. This is a book, not a paper. A book does not present new research outcomes.
%The rediscovery of back-propagation caused a rebirth in machine learning research, it allows perceptron to be extended to multi-layer neural network \citep{58337}. 
The invention of backpropagation allowed perceptron to be extended to multi-layer neural networks \citep{58337}.
As the first significant application, \citet{NIPS1989293} applied backward propagation to train computers to read human handwriting. 
The recent advances in computer hardware made the backward propagation algorithm feasible for training large and deep neural networks. 
Consequently, neural networks became more prevalent. 
Recently, researchers in this field discovered many empirical and heuristic techniques to improve the performance of neural networks in accuracy and convergence rate. These techniques include the modules applied in our benchmark model introduced in the first paragraph.
%Xavier initialisation \citep{pmlr-v9-glorot10a}, 
%ReLU activation functions \citep{relu-hahnloser}, 
%dropout \citep{DBLP:journals/corr/abs-1207-0580}, 
%momentum method \citep{rumelhart1986learning}, 
%weight decay \citep{NIPS1991563}, 
%mini-batch training \citep{DBLP:journals/corr/GoyalDGNWKTJH17},
%and batch normalisation \citep{pmlr-v37-ioffe15}.

%is designed to help on the sparsity of the output data;
%Last but not least,and  both improves the speed of convergence and computation.

%is invented to prevent over-fitting
%it is a technique to suppress the noise and improve generalisation for neural network model;
%is widely used to accelerate the training process and avoid slow convergence

% - Why is the study important? 
% again these are about how neural network and hence moved to methods
% end to end structure
% find latent feature
% ??
% ?? refer to the mid test
Neural networks are now widely applicable in many fields such as finance~\citep{Azoff:1994:NNT:561452,KAASTRA1996215}, facial recognition \citep{554195,655647}, and more recently natural language processing~\citep{Collobert:2008:UAN:1390156.1390177,Collobert:2011:NLP:1953048.2078186}.  It is irreplaceable by other machine learning models because of its distinguishing features. 

For example, neural network can accurately classify complex and highly non-linear dataset. The structure of nonlinearity behind artificial intelligence problems is usually difficult to formulate. Unlike classic statistical models, neural networks are able to autonomously learn the nonlinearity from complex data and extract latent features \citep{Bishop:2006:PRM:1162264}. To increase the complexity of the hypothesis class (i.e. the number of latent features), we can conveniently raise either the number of layers or the number of hidden neurons per layer.  
With a suitable size of the  hypothesis class, the neural network model is capable to extract latent features from complex datasets.

Moreover, the training process of neural network can efficiently utilise the computational power from modern multi-core CPUs and GPUs. The most computationally intensive part of training a neural network is to compute matrix multiplications and Hadamard product for large matrices. Modern languages like \texttt{Python} perform these matrix operations very effectively with highly vectorised code \citep{5725236}, through either CPU or GPU. Apart from vectorised calculations, packages like \texttt{Numpy} is also optimised to avoid copying data in memory, and to minimise the operation counts while performing matrices operations \citep{5725236}. These features of modern language allow the training neural network becomes more efficient.

% Draft Completed - Pending Review
The rest of this report firstly introduces the loss function, the modules applied and optimisation techniques. 
This is followed by the experiment setup and detailed simulation results.
Finally, we discuss importance of all the modules applied and how our neural network model benefits from them.
