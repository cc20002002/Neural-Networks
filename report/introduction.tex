\section{Introduction\label{chapter1}}
%Briefly introduce \textsc{nmf}, applications

%- Whatâ€™s the aim of the study? 
% Draft Completed - Pending Review
The aim of this study is to build a multi-layer neural network application to perform a ten-class classification and use various state-of-the-art modules to improve the prediction accuracy on the given dataset.
The task provides a labelled training dataset and an unlabelled testing dataset.
The training dataset given for this task consists of $128$ features for $60,000$ samples,
the test dataset contains $10,000$ samples with same amount of features as training dataset,
Given the naive multi-layer neural network implementation in the early stage of our research, %I don't understand this sentence
our neural network includes various state-of-the-art modules to further improve the prediction accuracy and convergence rate. These methods include advanced activation functions, 
weight decay, 
momentum method,
%stochastic gradient descent methods, 
mini-batch training, batch normalisation and dropout. 
Our model successfully classifies the cross validation (CV) data set with an accuracy of $89.9\%$ in $2.3$ minutes. 
To illustrate the effects of each module, we turn the module off one by one in our benchmark model and then explore the numerical consequences. 

% - Why is the study important? 
% again these are about how neural network and hence moved to methods
% end to end structure
% find latent feature
% ??
% ?? refer to the mid test
There are four main reasons that we think this study is important.
Firstly, neural network has the ability to generalise and classify the data that it has never seen before, hence it can help us predict the labels for testing dataset.
Secondly, we are given a training dataset and its labels with complex and non-linear patterns, neural network helps us find latent features in this large amounts of data,
in our experiment we use non-linear activation functions to help neurons to map these complex input features to find hidden patterns. 
Thirdly, neural network can utilise a parallel processing structure with large numbers of neurons, we can save massive time performing training models and predicting the testing dataset using vectorised matrix algebra operations, we also use stochastic gradient descent to find the optimal weights in this parallel processing structure.
Lastly, neural network is fault tolerant and self-repair since it ensures reliability when some portions of the network are not working, hence we use dropout in our experiment to make the neural network more resilient.
As we identify the importance of the study so we will focus on identifying the best neural network and optimisation methodologies for this purpose.

%Literature review find a review paper read and rewrite
The multi-class neural network classification is not a trivial extension from two-class neural networks,
it requires robust, adaptive, non-parametric classifiers that can be implemented on high-speed parallel computers,
the most commonly used classifier is the hyper-plane classifiers that form complex output regions such as sigmoid or tanh,
these classifiers does not require high memory and computation complexity but they require long training time,
they include multi-layer neurons trained with back-propagation \citet{OU20074}.
Gradient-based back propagation is introduced to minimise the loss between actual outputs and the predicted output in the supervised learning \citet{lecun1998Gradient},
We also found multiple strategies to optimise the mini-batch gradient descent using momentum, batch normalisation and early stopping,
these strategies further improves the training accuracy and achieve fast convergence for training the neural network model \citet{ruder2016overview}.

% Draft Completed - Pending Review
This report firstly introduce the theoretical supports for loss functions, various layers and optimisation techniques. 
This is followed by the experiment setup. We then compare the simulation results in a great detail.
Finally, we discuss importance of all these module and how our neural network model benefits from them.
