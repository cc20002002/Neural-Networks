\section{Related work}\label{chp3}
Researchers proposed many \textsc{nmf} algorithms. As the objective function of \textsc{nmf} is non-convex, for which the traditional gradient decent method could be very sensitive to step sizes and converge slowly, \citet{lee2001algorithms} first propose to algorithms which minimise Euclidean distance or Kullback-Leibler divergence~(\textsc{klnmf}) between the original matrix and its approximation. Although these algorithms are easy to implement and have reasonable convergent rate \citep{lee2001algorithms}, they require more iterations than alternatives such as gradient descent \citep{berry2007algorithms}. Also, the algorithms may fail on seriously corrupted datasets which violate its assumption of Gaussian noise or Poisson noise, respectively \citep{guan2017truncated}. Moreover, \citet{yang2011kullback} indicate that these methods are sensitive to the initial selection of matrices~$W$ and~$H$. The algorithms require many iterations to retrieve from poorly selected initial values.

Apart from different loss functions, several optimisation methods were proposed to improve the performance of \textsc{nmf}. After \citet{lee2001algorithms} proposed multiplicative update rules \textsc{mur}, \citet{ lin2007convergence} proposed a modified \textsc{mur} which guaranteed the convergence to a stationary point. This modified \textsc{mur}, however, did not improve the convergence rate of traditional \textsc{mur} \citep{guan2012nenmf}. Moreover, as \textsc{mur} does not impose sparseness, \citet{berry2007algorithms} proposed a projected nonnegative least square (\textsc{pnls}) method to enforce sparseness. In each nonnegative least square sub-problem, this algorithm projects the negative elements of least squares solution directly to zero. Nevertheless, \textsc{pnls} does not guarantee convergence \citep{guan2012nenmf}. 

In contrast to these gradient-based optimisation methods, \citet{kim2008nonnegative} presented an active set method which divides variables into two sets, a free set and an active set. They update free set in each iteration by solving an unconstrained equation. Even though the active set method has a nice convergence rate, it assumes strictly convexity in each nonnegative least square sub-problem \citep{kim2008nonnegative}. These assumptions are easily violated in real life applications.

There exist many robust \textsc{nmf} algorithms which include penalties in the objective functions. For example, \citet{lam2008non} proposes ${L_1}$-norm based \textsc{nmf} to model noisy data with a Laplace distribution which is less sensitive to outliers. However, as $L_1$-norm is not differentiable at zero, the optimisation procedure is computationally expensive. \citet{kong2011robust} proposed an \textsc{nmf} algorithm using $L_{21}$-norm loss function which is more stable. The updating rules used in $L_{21}$-norm \textsc{nmf}, however, still converge slowly because of continual use of the power method \citep{guan2017truncated}.


